{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "This is a notebook for calculating the following statistics in the iceplant map:\n",
    "- producer's accuracies and user's accuracies by class,\n",
    "- overall accuracy, and\n",
    "- area estimates. \n",
    "\n",
    "Each statistic is accompanied by a 95% confidence interval. \n",
    "To do this, we follow the notation and calculations in Olofsson et al., 2014.\n",
    "\n",
    "The notebook uses the following files:\n",
    "1. A CSV file of the sampled points used for the accuracy assessment (each point is a row) with two columns: `map_class` and `ref_class`. The `map_class` column is the classification of a given point in the map, while `ref_class` is the \\\"ground truth\\\" classification of the point.\n",
    "\n",
    "2. A CSV file with the number of pixels per class in the map.\n",
    "\n",
    "## References\n",
    "\n",
    "Pontus Olofsson, Giles M. Foody, Martin Herold, Stephen V. Stehman, Curtis E. Woodcock, Michael A. Wulder, Good practices for estimating area and assessing accuracy of land change, Remote Sensing of Environment,Volume 148, 2014, Pages 42-57, ISSN 0034-4257, https://doi.org/10.1016/j.rse.2014.02.015.  (https://www.sciencedirect.com/science/article/pii/S0034425714000704)\n",
    "\n",
    "Perrot, M., & Duchesnay, É. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825--2830."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Assuming repository's parent directory is the Documents directory\n",
    "home = os.path.expanduser(\"~\")\n",
    "os.chdir(os.path.join(home,'Documents','iceplant-detection-santa-barbara'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel counts per class:\n",
      "class 1:  120,173,466\n",
      "class 2:    5,981,423\n",
      "class 3:  188,071,487\n"
     ]
    }
   ],
   "source": [
    "# number of classes in the map\n",
    "n_classes = 3\n",
    "\n",
    "validation_data_dir = os.path.join(os.getcwd(),\n",
    "                              'data',\n",
    "                              'validation_data')\n",
    "\n",
    "# load csv with validation points\n",
    "df = pd.read_csv(os.path.join(validation_data_dir,\n",
    "                              'final_model_map_and_reference_classes.csv'))\n",
    "\n",
    "# load counts of pixels per class in map\n",
    "pixel_count_path = os.path.join(validation_data_dir,\n",
    "                              'map_pixel_counts',\n",
    "                              'final_model_combined_pixel_counts_total.csv')\n",
    "\n",
    "pix_counts = pd.read_csv(pixel_count_path)\n",
    "pix_counts = pix_counts.to_numpy()[0]\n",
    "\n",
    "print(\"Pixel counts per class:\")\n",
    "for n_pix, i  in zip(pix_counts, range(n_classes)):\n",
    "    print(f'class {i+1}: {pix_counts[i]:>12,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points in each reference class\n",
      "class 1: 321\n",
      "class 2: 264\n",
      "class 3: 404\n",
      "\n",
      "\n",
      "Points in each map class\n",
      "class 1: 289\n",
      "class 2: 296\n",
      "class 3: 404\n"
     ]
    }
   ],
   "source": [
    "# counts by reference class\n",
    "print('Points in each reference class')\n",
    "counts = np.unique(df.ref_class, return_counts=True)\n",
    "for i, n_pts in zip(counts[0],counts[1]):\n",
    "    print(f'class {i+1}: {n_pts}')\n",
    "print('\\n')\n",
    "\n",
    "# counts by map class: these should match the counts given by the stratified sample design\n",
    "print('Points in each map class')\n",
    "counts = np.unique(df.map_class, return_counts=True)\n",
    "for i, n_pts in zip(counts[0],counts[1]):\n",
    "    print(f'class {i+1}: {n_pts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Here we create a confusion matrix $n$ such that \n",
    "\n",
    "$n_{i,j}$ = number of points predicted as $i$, known to be $j$, \n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "$n_{i,j}$ = number of points that have map class as $i$ and reference class $j$.\n",
    "\n",
    "To do this, we use the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function from the scikit-learn Python package (Perrot & Duchesnay, 2011). \n",
    "The matrix $C$ obtained with `confusion_matrix` is such that\n",
    "\n",
    "$C_{i,j}$ = points known to belong to class $i$ and are \n",
    "predicted as class  $j$.\n",
    "\n",
    "However, the notation in the Olofsson et al. paper is\n",
    "\n",
    "$n_{i,j}$ = number of points predicted as class $i$, known to be class $j$,\n",
    "\n",
    "so we need to take the transpose of $C$ to match the notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[247,   2,  40],\n",
       "       [ 26, 262,   8],\n",
       "       [ 48,   0, 356]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate confusion matrix pro imported data\n",
    "n = confusion_matrix(df.ref_class, df.map_class, labels=range(n_classes)).T\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User's accuracies with 95% confidence interval:\n",
      "class 1: 85.47 ± 4.07\n",
      "class 2: 88.51 ± 3.64\n",
      "class 3: 88.12 ± 3.16\n"
     ]
    }
   ],
   "source": [
    "# points in sample that had class i in map (predicted as i, any true class j)\n",
    "n_idot = [sum(n[i,:]) for i in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "# estimated user's accuracy\n",
    "U_hat = [n[i,i] / n_idot[i] for i in range(n_classes)]\n",
    "\n",
    "# variance of estimated user's accuracy\n",
    "var_U_hat = [U_hat[i] * (1-U_hat[i])/(n_idot[i]-1) for i in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "print(\"User's accuracies with 95% confidence interval:\")\n",
    "for U_hati, var_i, i  in zip(U_hat, var_U_hat, range(n_classes)):\n",
    "    print(f'class {i+1}: {U_hati*100:.2f} ± {1.96 * np.sqrt(var_i)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy with 95% confidence interval:\n",
      "87.11 ± 2.45\n"
     ]
    }
   ],
   "source": [
    "# total number of pixels in the map\n",
    "total_pix = sum(pix_counts)\n",
    "\n",
    "# fractions of area in map classified in each class\n",
    "W = [pix_counts[i]/ total_pix for i in range(n_classes)]      \n",
    "\n",
    "# -------------------------------------\n",
    "# estimated overall accuracy\n",
    "O_hat = sum([W[i]*n[i,i]/n_idot[i] for i in range(n_classes)])\n",
    "\n",
    "# -------------------------------------\n",
    "# variance of estimated overall accuracy\n",
    "var_O_hat = sum([ W[i]**2 * U_hat[i] * (1-U_hat[i])/(n_idot[i]-1) for i in range(n_classes)])\n",
    "\n",
    "# -------------------------------------\n",
    "print('Overall accuracy with 95% confidence interval:')\n",
    "print(f'{O_hat*100:.2f} ± {1.96*np.sqrt(var_O_hat)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer's Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer's accuracies with 95% confidence interval:\n",
      "class 1: 81.79 ± 3.94\n",
      "class 2: 86.42 ± 16.24\n",
      "class 3: 90.80 ± 2.40\n"
     ]
    }
   ],
   "source": [
    "# estimated fraction of area in class j (Olofsson et al., eq. 9) \n",
    "p_hat_dotj = []\n",
    "for j in range(n_classes):\n",
    "    p_hat_ij = [ W[i]*n[i,j]/n_idot[i] for i in range(n_classes) ]\n",
    "    p_hat_dotj.append(sum(p_hat_ij))  \n",
    "    \n",
    "# -------------------------------------\n",
    "# estimated producer's accuracy \n",
    "P_hat = [ (W[j]*n[j,j]/n_idot[j]) / p_hat_dotj[j] for j in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "# variance of estimated producer's accuracy \n",
    "# notice N_jdot is pix_counts[j]\n",
    "N_hat_dotj = []\n",
    "for j in range(n_classes):\n",
    "    summands = [ pix_counts[i] * n[i,j]/n_idot[i] for i in range(n_classes)]\n",
    "    N_hat_dotj.append(sum(summands))\n",
    "\n",
    "summand1 = []\n",
    "for j in range(n_classes):\n",
    "    summand1.append((pix_counts[j]**2) * ((1-P_hat[j])**2) * U_hat[j] * (1-U_hat[j]) / (n_idot[j] - 1))\n",
    "\n",
    "summand2 = []\n",
    "for j in range(n_classes):\n",
    "    inner = []\n",
    "    for i in range(n_classes):\n",
    "        if i!=j:\n",
    "            inner.append( (pix_counts[i]**2) * (n[i,j])/(n_idot[i]) * ( 1 - n[i,j]/n_idot[i])/(n_idot[i]-1)) \n",
    "    summand2.append((P_hat[j]**2) * sum(inner))\n",
    "\n",
    "var_P_hat = [1/(N_hat_dotj[j]**2) *  (summand1[j] + summand2[j]) for j in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "print(\"Producer's accuracies with 95% confidence interval:\")\n",
    "for P_hati, var_i, i  in zip(P_hat, var_P_hat, range(n_classes)):\n",
    "    print(f'class {i+1}: {P_hati*100:.2f} ± {1.96 * np.sqrt(var_i)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Area Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage area per class with 95% confidence interval:\n",
      "class 1: 39.96 ± 2.45\n",
      "class 2: 1.95 ± 0.37\n",
      "class 3: 58.09 ± 2.43\n"
     ]
    }
   ],
   "source": [
    "# standard error of estimated area per class\n",
    "S_p_hat_dotj = []\n",
    "for j in range(n_classes):\n",
    "    summands = [ (W[i]**2) * (n[i,j]/n_idot[i]) * (1 -  (n[i,j]/n_idot[i]))/ (n_idot[i]-1) \n",
    "                for i in range(n_classes)]\n",
    "    S_p_hat_dotj.append(np.sqrt(sum(summands)))\n",
    "    \n",
    "# -------------------------------------\n",
    "print(\"Percentage area per class with 95% confidence interval:\")\n",
    "for perc_i, err_i, i  in zip(p_hat_dotj, S_p_hat_dotj, range(n_classes)):\n",
    "    print(f'class {i+1}: {perc_i*100:.2f} ± {1.96 * err_i*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated area per class (km^2) with 95% confidence interval:\n",
      "class 1: 45.21 ± 2.77\n",
      "class 2: 2.21 ± 0.42\n",
      "class 3: 65.71 ± 2.75\n"
     ]
    }
   ],
   "source": [
    "# area per pixel in m^2\n",
    "pixel_area_m2 = 0.6**2\n",
    "\n",
    "# total map area in km^2 (1 m^2 = 1/10^6 ha)\n",
    "map_area_ha = total_pix * pixel_area_m2 / 10**6\n",
    "\n",
    "approx_area_per_class = [map_area_ha * p_hat_dotj[i] for i in range(n_classes)]\n",
    "\n",
    "# standard error area per class\n",
    "SE_area_per_class = [map_area_ha * S_p_hat_dotj[i] for i in range(n_classes)]\n",
    "\n",
    "# -------------------------------------\n",
    "print(\"Estimated area per class (km^2) with 95% confidence interval:\")\n",
    "for area_i, err_i, i  in zip(approx_area_per_class, SE_area_per_class, range(n_classes)):\n",
    "    print(f'class {i+1}: {area_i:,.2f} ± {1.96 * err_i:,.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('mpc-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d9c35c8115062f8f91024dabb290da02183a26877d6f60ace8c62884141c720"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
